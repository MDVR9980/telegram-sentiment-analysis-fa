{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9039f123",
   "metadata": {},
   "source": [
    "# Full Scale Sentiment Analysis (Stateless & Robust)\n",
    "## **Course:** Advanced NLP (Fall 2025)\n",
    "\n",
    "## This notebook implements a production-grade pipeline designed for large datasets (950k+ posts).\n",
    "## Key Features:\n",
    "### 1. **Stateless Inference:** Each post is processed independently. The LLM retains zero memory of previous posts.\n",
    "### 2. **Robust Checkpointing:** Results are saved incrementally. If the system crashes, execution resumes automatically from the last saved row.\n",
    "### 3. **Memory Efficient:** Processes data in batches to respect RAM limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ollama\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb38b00",
   "metadata": {},
   "source": [
    "## Add scripts path for modular imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from scripts.preprocessor import TextPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82eba7",
   "metadata": {},
   "source": [
    "## Visualization Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e66b7",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef05ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "PROCESSED_DIR = \"../data/processed\"\n",
    "RESULTS_DIR = \"../results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787dea7c",
   "metadata": {},
   "source": [
    "### Create directories if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c866354",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3c529",
   "metadata": {},
   "source": [
    "### Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3275d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DATA_FILE = os.path.join(PROCESSED_DIR, \"master_cleaned_dataset.csv\")\n",
    "FINAL_RESULTS_FILE = os.path.join(PROCESSED_DIR, \"final_sentiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911df2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3\"\n",
    "BATCH_SIZE = 100  # Save to disk every 100 posts\n",
    "SOURCES = [\"kafiha\", \"TweetyChannel\", \"radiofarda\", \"iranintlTV\", \"bbcpersian\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145c813a",
   "metadata": {},
   "source": [
    "## 2. Data Preparation (Load, Clean, Filter)\n",
    "### To ensure indices align perfectly for resuming, we first create a **Master Cleaned Dataset**.\n",
    "### If this file already exists, we skip this step to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb32c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(MASTER_DATA_FILE):\n",
    "    print(f\"âœ… Master dataset found at {MASTER_DATA_FILE}. Loading...\")\n",
    "    full_df = pd.read_csv(MASTER_DATA_FILE)\n",
    "else:\n",
    "    print(\"âš ï¸ Master dataset not found. Building from scratch...\")\n",
    "    \n",
    "    # 1. Load Raw Data\n",
    "    all_dfs = []\n",
    "    for source in SOURCES:\n",
    "        path = os.path.join(DATA_DIR, f\"{source}_messages.csv\")\n",
    "        if os.path.exists(path):\n",
    "            print(f\"   - Loading {source}...\")\n",
    "            temp_df = pd.read_csv(path)\n",
    "            temp_df['source'] = source\n",
    "            all_dfs.append(temp_df)\n",
    "    \n",
    "    if not all_dfs:\n",
    "        raise FileNotFoundError(\"No source CSV files found in data/ directory!\")\n",
    "        \n",
    "    full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # 2. Preprocessing\n",
    "    print(\"   - Initializing Preprocessor...\")\n",
    "    preprocessor = TextPreprocessor()\n",
    "    \n",
    "    # Fill NaNs\n",
    "    full_df['text'] = full_df['text'].fillna('')\n",
    "    full_df['emoji_reactions'] = full_df['emoji_reactions'].fillna('')\n",
    "    \n",
    "    print(\"   - Cleaning text (This might take a few minutes for 900k+ rows)...\")\n",
    "    tqdm.pandas(desc=\"Cleaning\")\n",
    "    full_df['clean_post_text'] = full_df['text'].progress_apply(preprocessor.clean_text)\n",
    "    full_df['clean_reactions'] = full_df['emoji_reactions'].apply(preprocessor.clean_reactions)\n",
    "    \n",
    "    # 3. Filtering (Remove short/empty texts)\n",
    "    initial_len = len(full_df)\n",
    "    full_df = full_df[full_df['clean_post_text'].str.len() > 10].reset_index(drop=True)\n",
    "    print(f\"   - Filtered {initial_len - len(full_df)} short/empty posts.\")\n",
    "    \n",
    "    # 4. Save Master File\n",
    "    print(f\"   - Saving Master Dataset ({len(full_df)} rows)...\")\n",
    "    full_df.to_csv(MASTER_DATA_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"ğŸš€ Ready to process {len(full_df)} posts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f86f4",
   "metadata": {},
   "source": [
    "## 3. Stateless Inference Engine\n",
    "### We define the logic to query Llama 3.\n",
    "### **Important:** `messages` list is recreated inside the function every time to ensure **Zero Memory** of previous posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_label(text, reactions):\n",
    "    \"\"\"\n",
    "    Stateless Sentiment Analysis using Few-Shot Prompting.\n",
    "    Each call is independent. No context is shared between posts.\n",
    "    \"\"\"\n",
    "    prompt_text = f\"Ù…ØªÙ† Ù¾Ø³Øª: {text}\\nÙˆØ§Ú©Ù†Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†: {reactions}\"\n",
    "    \n",
    "    # Optimized Few-Shot Prompt\n",
    "    system_prompt = (\n",
    "        \"ØªÙˆ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ø¯Ù‚ÛŒÙ‚ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ. Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ±ØŒ Ù¾Ø³Øª Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†.\\n\"\n",
    "        \"Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§: ['Ø®ÙˆØ´Ø­Ø§Ù„', 'Ù†Ø§Ø±Ø§Ø­Øª', 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'Ù…Ø¶Ø·Ø±Ø¨', 'Ø®Ù†Ø«ÛŒ', 'Ù†Ú¯Ø±Ø§Ù†']\\n\\n\"\n",
    "        \"--- Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ ---\\n\"\n",
    "        \"Ù…Ø«Ø§Ù„ Û± (ØªØ¨Ù„ÛŒØºØ§Øª/Ø®Ø¨Ø±): ØªÙˆØ± Ø§Ø³ØªØ§Ù†Ø¨ÙˆÙ„ ÙˆÛŒÚ˜Ù‡ Ø¨Ù„Ú© ÙØ±Ø§ÛŒØ¯ÛŒ. -> Ø®Ù†Ø«ÛŒ\\n\"\n",
    "        \"Ù…Ø«Ø§Ù„ Û² (ØºÙ…/Ø¯Ù„ØªÙ†Ú¯ÛŒ): ÙØ±Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù†Øª Ø³Ø®ØªÙ‡ Ø¹Ø²ÛŒØ²Ù…. -> Ù†Ø§Ø±Ø§Ø­Øª\\n\"\n",
    "        \"Ù…Ø«Ø§Ù„ Û³ (Ø·Ù†Ø²/Ù…ÙˆÙÙ‚ÛŒØª): ÙˆØ§ÛŒ Ú†Ù‚Ø¯Ø± Ø®ÙˆØ´Ø­Ø§Ù„Ù… ØªÙ…ÙˆÙ… Ø´Ø¯. -> Ø®ÙˆØ´Ø­Ø§Ù„\\n\"\n",
    "        \"Ù…Ø«Ø§Ù„ Û´ (Ø®Ø´Ù…/Ø§Ø¹ØªØ±Ø§Ø¶): Ú†Ø±Ø§ ÙˆØ¶Ø¹ÛŒØª Ø§ÛŒÙ†ØªØ±Ù†Øª Ø§ÛŒÙ†Ø·ÙˆØ±ÛŒÙ‡ØŸ -> Ø¹ØµØ¨Ø§Ù†ÛŒ\\n\"\n",
    "        \"Ù…Ø«Ø§Ù„ Ûµ (Ù†Ú¯Ø±Ø§Ù†ÛŒ/Ù¾ÙˆÙ„): Ø¢Ø®Ø± Ù…Ø§Ù‡ Ø´Ø¯ Ùˆ Ø­Ù‚ÙˆÙ‚ Ù†Ø±ÛŒØ®ØªÙ†. -> Ù†Ú¯Ø±Ø§Ù†\\n\"\n",
    "        \"--- Ù¾Ø§ÛŒØ§Ù† Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ ---\\n\"\n",
    "        \"ÙÙ‚Ø· Ø¨Ø±Ú†Ø³Ø¨ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Generate a fresh request (Stateless)\n",
    "        response = ollama.chat(model=MODEL_NAME, messages=[\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': prompt_text},\n",
    "        ])\n",
    "        \n",
    "        label = response['message']['content'].strip()\n",
    "        \n",
    "        # Basic Validation\n",
    "        valid_labels = ['Ø®ÙˆØ´Ø­Ø§Ù„', 'Ù†Ø§Ø±Ø§Ø­Øª', 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'Ù…Ø¶Ø·Ø±Ø¨', 'Ø®Ù†Ø«ÛŒ', 'Ù†Ú¯Ø±Ø§Ù†']\n",
    "        for v in valid_labels:\n",
    "            if v in label: return v\n",
    "        return \"Ø®Ù†Ø«ÛŒ\" # Fallback\n",
    "    except Exception as e:\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12775e54",
   "metadata": {},
   "source": [
    "## 4. Main Execution Loop (With Resume Capability)\n",
    "### We check `final_sentiment_results.csv`. If it exists, we count the rows and skip that many in `full_df`.\n",
    "\n",
    "#### 1. Check Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d5aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(FINAL_RESULTS_FILE):\n",
    "    # Count lines in existing file to determine start index\n",
    "    # We substract 1 for the header\n",
    "    with open(FINAL_RESULTS_FILE, 'r', encoding='utf-8-sig') as f:\n",
    "        processed_count = sum(1 for _ in f) - 1\n",
    "    \n",
    "    # Determine mode\n",
    "    write_mode = 'a' # Append\n",
    "    header = False   # Don't write header again\n",
    "    start_index = processed_count\n",
    "    print(f\"ğŸ”„ Resuming from index {start_index}...\")\n",
    "else:\n",
    "    write_mode = 'w' # Write new\n",
    "    header = True    # Write header\n",
    "    start_index = 0\n",
    "    print(\"ğŸŸ¢ Starting new analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01cdcd",
   "metadata": {},
   "source": [
    "#### 2. Inference Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_index < len(full_df):\n",
    "    batch_buffer = []\n",
    "    \n",
    "    # Iterate from the last checkpoint\n",
    "    for i in tqdm(range(start_index, len(full_df)), desc=\"Processing\", initial=start_index, total=len(full_df)):\n",
    "        \n",
    "        row = full_df.iloc[i]\n",
    "        \n",
    "        # Call LLM (Stateless)\n",
    "        sentiment = get_sentiment_label(row['clean_post_text'], row['clean_reactions'])\n",
    "        \n",
    "        # Prepare Result Row\n",
    "        result_dict = {\n",
    "            'msg_id': row.get('msg_id'),\n",
    "            'date': row.get('date'),\n",
    "            'source': row.get('source'),\n",
    "            'clean_text': row['clean_post_text'], # Saving cleaned text to save space\n",
    "            'sentiment': sentiment\n",
    "        }\n",
    "        batch_buffer.append(result_dict)\n",
    "        \n",
    "        # 3. Save to Disk (Batch)\n",
    "        if len(batch_buffer) >= BATCH_SIZE:\n",
    "            df_batch = pd.DataFrame(batch_buffer)\n",
    "            df_batch.to_csv(FINAL_RESULTS_FILE, mode=write_mode, header=header, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            # Reset buffer and flags\n",
    "            batch_buffer = []\n",
    "            header = False # Never write header after first batch\n",
    "            write_mode = 'a' # Always append hereafter\n",
    "            \n",
    "    # 4. Save remaining items\n",
    "    if batch_buffer:\n",
    "        df_batch = pd.DataFrame(batch_buffer)\n",
    "        df_batch.to_csv(FINAL_RESULTS_FILE, mode=write_mode, header=header, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(\"\\nâœ… Analysis Complete! All rows processed.\")\n",
    "else:\n",
    "    print(\"\\nâœ… All rows were already processed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dead0",
   "metadata": {},
   "source": [
    "### 5. Visualization & Statistics\n",
    "#### Now we load the FINAL processed file and generate the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaedefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading final results for visualization...\")\n",
    "df_results = pd.read_csv(FINAL_RESULTS_FILE)\n",
    "df_results['date'] = pd.to_datetime(df_results['date'])\n",
    "df_results['month_year'] = df_results['date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d5adb4",
   "metadata": {},
   "source": [
    "## --- A. Trend Analysis (Time Series) ---\n",
    "### Group by Month and Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_data = df_results.groupby(['month_year', 'sentiment']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f299620",
   "metadata": {},
   "source": [
    "#### Filter for top 4 active sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['Ø®ÙˆØ´Ø­Ø§Ù„', 'Ù†Ø§Ø±Ø§Ø­Øª', 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'Ù†Ú¯Ø±Ø§Ù†']\n",
    "existing_cols = [c for c in target_cols if c in trend_data.columns]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "trend_data[existing_cols].plot(kind='line', marker='o', figsize=(14, 7))\n",
    "plt.title(\"Sentiment Trends Over 5 Years (Monthly)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Post Count\")\n",
    "plt.legend(title=\"Sentiment\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"final_trend_analysis.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aaca8",
   "metadata": {},
   "source": [
    "## --- B. Hope vs. Despair (Ratio) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f02142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_mood(sent):\n",
    "    if sent == 'Ø®ÙˆØ´Ø­Ø§Ù„': return 'Hope/Positive'\n",
    "    if sent in ['Ù†Ø§Ø±Ø§Ø­Øª', 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'Ù†Ú¯Ø±Ø§Ù†', 'Ù…Ø¶Ø·Ø±Ø¨']: return 'Despair/Negative'\n",
    "    return 'Neutral'\n",
    "\n",
    "df_results['mood_category'] = df_results['sentiment'].apply(categorize_mood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7f87b",
   "metadata": {},
   "source": [
    "### Calculate percentages per source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa23f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_stats = df_results.groupby(['source', 'mood_category']).size().unstack(fill_value=0)\n",
    "mood_stats['Total'] = mood_stats.sum(axis=1)\n",
    "mood_stats['Despair_Rate'] = (mood_stats.get('Despair/Negative', 0) / mood_stats['Total'] * 100).round(1)\n",
    "\n",
    "print(\"\\n--- Mood Statistics per Channel ---\")\n",
    "display(mood_stats)\n",
    "mood_stats.to_csv(os.path.join(RESULTS_DIR, \"final_mood_statistics.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907bcdb",
   "metadata": {},
   "source": [
    "### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72710a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_stats[['Hope/Positive', 'Despair/Negative', 'Neutral']].plot(kind='bar', stacked=True, color=['green', 'red', 'gray'], figsize=(10, 6))\n",
    "plt.title(\"Sentiment Distribution by Channel\")\n",
    "plt.ylabel(\"Number of Posts\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"final_mood_barchart.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
