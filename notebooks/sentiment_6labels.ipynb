{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6521bd-3c26-4150-ad8d-82cacdfbf104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdvr0480/anaconda3/envs/sentiment_telegram/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 - imports and paths\n",
    "# English comments inside code as requested.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Paths - adjust if needed\n",
    "ROOT = \"/media/mdvr0480/Data1/Uni/Term1/ANLP/telegram-sentiment-analysis-fa\"\n",
    "MODEL_DIR = os.path.join(ROOT, \"models\", \"SeyedAli-Persian-Text-Emotion-Bert-V1\")\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "OUTPUT_DIR = os.path.join(DATA_DIR, \"sentiment_results_batch\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469128d-2cb0-4b9c-a7c8-8d037a1368b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - labels and helper functions\n",
    "\n",
    "# Emotion labels as required by the assignment\n",
    "EMOTION_LABELS = [\"خوشحال\", \"ناراحت\", \"عصبانی\", \"نگران\", \"مضطرب\", \"خنثی\"]\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Simple softmax for numpy array / torch tensor\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        x = x.detach().cpu().numpy()\n",
    "    e = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e / e.sum(axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19764f0d-2934-4ac5-8ef8-259dc3a34095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - text cleaning functions (English comments)\n",
    "\n",
    "# Try to use hazm Normalizer if available for better Persian normalization\n",
    "try:\n",
    "    from hazm import Normalizer\n",
    "    hazm_normalizer = Normalizer()\n",
    "except Exception:\n",
    "    hazm_normalizer = None\n",
    "    print(\"hazm not available or failed to import - will use regex fallback normalizer\")\n",
    "\n",
    "# Regex patterns\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "EMOJI_RE = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def basic_persian_normalize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic normalization for Persian text:\n",
    "    - remove HTML, URLs, emojis\n",
    "    - remove extra whitespace\n",
    "    - optionally use hazm Normalizer for advanced normalization\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    t = text\n",
    "    t = re.sub(r'\\u200c', ' ', t)  # ZWNJ -> space\n",
    "    t = URL_RE.sub(' ', t)\n",
    "    t = HTML_TAG_RE.sub(' ', t)\n",
    "    t = EMOJI_RE.sub(' ', t)\n",
    "    # remove non-printable/control\n",
    "    t = re.sub(r'[\\r\\n\\t]+', ' ', t)\n",
    "    # remove numbers and excessive punctuation (but keep basic punctuation)\n",
    "    t = re.sub(r'[0-9۰-۹]+', ' ', t)\n",
    "    # remove redundant punctuation (more than 2)\n",
    "    t = re.sub(r'([!?.,؛،])\\1+', r'\\1', t)\n",
    "    # remove other symbols\n",
    "    t = re.sub(r'[^آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهیءًٌٍَُِْٔٔٔ\\s\\.\\,\\?\\!\\،\\؛\\-]', ' ', t)\n",
    "    # collapse spaces\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    # hazm normalization if available\n",
    "    if hazm_normalizer:\n",
    "        try:\n",
    "            t = hazm_normalizer.normalize(t)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169d114-a73c-47fb-b9c0-0152822369d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - read CSVs from data directory and concat them\n",
    "# We assume message column contains the post text.\n",
    "csv_files = sorted(glob(os.path.join(DATA_DIR, \"*_messages.csv\")))\n",
    "print(\"Found CSV files:\", csv_files)\n",
    "\n",
    "dfs = []\n",
    "for fp in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        df[\"__source_file\"] = os.path.basename(fp)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read\", fp, e)\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    raise RuntimeError(\"No CSVs loaded - check DATA_DIR and file patterns.\")\n",
    "\n",
    "raw_df = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "print(\"Raw dataframe shape:\", raw_df.shape)\n",
    "\n",
    "# Save a copy of raw merged data\n",
    "raw_out = os.path.join(OUTPUT_DIR, \"raw_historical_posts.csv\")\n",
    "raw_df.to_csv(raw_out, index=False)\n",
    "print(\"Saved raw merged CSV to\", raw_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3aa830-01d4-41d6-86f1-00bb64b4b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - apply preprocessing\n",
    "# We'll create clean_post_text and (if reactions column exists) clean_reactions\n",
    "\n",
    "# Identify possible text columns\n",
    "text_cols = []\n",
    "for col in [\"message\", \"post_text\", \"text\", \"content\"]:\n",
    "    if col in raw_df.columns:\n",
    "        text_cols.append(col)\n",
    "# choose first available text column\n",
    "if len(text_cols) == 0:\n",
    "    # fallback: try to guess any column with many string values\n",
    "    for c in raw_df.columns:\n",
    "        if raw_df[c].dtype == object and raw_df[c].astype(str).str.len().mean() > 10:\n",
    "            text_cols.append(c)\n",
    "            break\n",
    "\n",
    "text_col = text_cols[0] if text_cols else None\n",
    "print(\"Using text column:\", text_col)\n",
    "\n",
    "def safe_get_date(df):\n",
    "    # try multiple column names for timestamp\n",
    "    for c in [\"date\", \"timestamp\", \"created_at\", \"time\"]:\n",
    "        if c in df.columns:\n",
    "            try:\n",
    "                dt = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "                return dt.rename(\"date\")\n",
    "            except:\n",
    "                continue\n",
    "    return pd.Series([pd.NaT]*len(df), name=\"date\")\n",
    "\n",
    "raw_df[\"clean_post_text\"] = raw_df[text_col].fillna(\"\").astype(str).map(basic_persian_normalize)\n",
    "\n",
    "# reactions: if present, collapse to string and clean\n",
    "reaction_cols = [c for c in [\"reactions\", \"comments\", \"reply_text\", \"replies\"] if c in raw_df.columns]\n",
    "if reaction_cols:\n",
    "    raw_df[\"raw_reactions\"] = raw_df[reaction_cols].astype(str).agg(\" || \".join, axis=1)\n",
    "    raw_df[\"clean_reactions\"] = raw_df[\"raw_reactions\"].map(basic_persian_normalize)\n",
    "else:\n",
    "    raw_df[\"raw_reactions\"] = \"\"\n",
    "    raw_df[\"clean_reactions\"] = \"\"\n",
    "\n",
    "# attach parsed date\n",
    "raw_df[\"date\"] = safe_get_date(raw_df)\n",
    "\n",
    "# Save preprocessed CSV\n",
    "processed_out = os.path.join(OUTPUT_DIR, \"preprocessed_posts.csv\")\n",
    "raw_df.to_csv(processed_out, index=False)\n",
    "print(\"Saved preprocessed CSV to\", processed_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a12990b-fca1-4234-bb41-f9aab03a9ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - load model and tokenizer from local path\n",
    "# We will use AutoTokenizer and AutoModelForSequenceClassification.\n",
    "# The model is expected to have num_labels = 6. If not, we'll still proceed but warn.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded. Num labels:\", model.config.num_labels)\n",
    "if model.config.num_labels != len(EMOTION_LABELS):\n",
    "    print(\"WARNING: model.num_labels != len(EMOTION_LABELS). Make sure label order matches training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad041f-313c-4aaf-9979-2b132ee1253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - batched prediction\n",
    "def predict_texts(texts, batch_size=32, max_length=256):\n",
    "    \"\"\"\n",
    "    Predict emotion probabilities for a list of texts.\n",
    "    Returns list of dicts: {label:score,...} for each text (labels in EMOTION_LABELS order).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n = len(texts)\n",
    "    for i in tqdm(range(0, n, batch_size), desc=\"Predicting batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch_texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc)\n",
    "            logits = out.logits  # shape (batch, num_labels)\n",
    "            probs = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        for p in probs:\n",
    "            # ensure mapping to our EMOTION_LABELS\n",
    "            d = {EMOTION_LABELS[j]: float(p[j]) for j in range(min(len(EMOTION_LABELS), p.shape[0]))}\n",
    "            results.append(d)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d448ba-fd75-419f-a060-a920fc37fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - run predictions on clean_post_text\n",
    "texts = raw_df[\"clean_post_text\"].fillna(\"\").tolist()\n",
    "pred_probs = predict_texts(texts, batch_size=64, max_length=256)\n",
    "\n",
    "# Build dataframe of predictions\n",
    "pred_df = pd.DataFrame(pred_probs)\n",
    "# choose top label\n",
    "pred_df[\"pred_label\"] = pred_df[EMOTION_LABELS].idxmax(axis=1)\n",
    "# combine with original dataframe\n",
    "final = pd.concat([raw_df.reset_index(drop=True), pred_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save detailed results (includes probabilities for each label)\n",
    "final_out = os.path.join(OUTPUT_DIR, \"sentiment_results_6labels_detailed.csv\")\n",
    "final.to_csv(final_out, index=False)\n",
    "print(\"Saved prediction results to\", final_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68eefc8-aebe-463e-9358-beaf676f388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 - time series aggregation and plots\n",
    "\n",
    "# ensure date column is datetime\n",
    "final[\"date\"] = pd.to_datetime(final[\"date\"], errors=\"coerce')\n",
    "\n",
    "# If date exists, aggregate monthly and yearly trends\n",
    "if final[\"date\"].notna().sum() > 0:\n",
    "    # monthly\n",
    "    final[\"month\"] = final[\"date\"].dt.to_period(\"M\")\n",
    "    monthly = final.groupby([\"month\", \"pred_label\"]).size().unstack(fill_value=0)\n",
    "    monthly.index = monthly.index.astype(str)\n",
    "    monthly_out = os.path.join(OUTPUT_DIR, \"monthly_emotion_counts.csv\")\n",
    "    monthly.to_csv(monthly_out)\n",
    "    print(\"Saved monthly counts to\", monthly_out)\n",
    "\n",
    "    # plot monthly trends (line plot)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    monthly.plot(figsize=(12,6))\n",
    "    plt.title(\"Monthly emotion trends\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Post counts\")\n",
    "    plt.legend(title=\"Emotion\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # yearly\n",
    "    final[\"year\"] = final[\"date\"].dt.to_period(\"Y\")\n",
    "    yearly = final.groupby([\"year\", \"pred_label\"]).size().unstack(fill_value=0)\n",
    "    yearly_out = os.path.join(OUTPUT_DIR, \"yearly_emotion_counts.csv\")\n",
    "    yearly.to_csv(yearly_out)\n",
    "    print(\"Saved yearly counts to\", yearly_out)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    yearly.plot(kind=\"bar\", stacked=True)\n",
    "    plt.title(\"Yearly emotion distribution\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Post counts\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid date column found; skipping time-series aggregation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aadc647-b05b-46d7-b234-92ca20c1b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 - per source (based on __source_file or any 'source' column)\n",
    "source_col = \"__source_file\" if \"__source_file\" in final.columns else (\"source\" if \"source\" in final.columns else None)\n",
    "if source_col:\n",
    "    per_source = final.groupby([source_col, \"pred_label\"]).size().unstack(fill_value=0)\n",
    "    per_source_out = os.path.join(OUTPUT_DIR, \"per_source_emotion_counts.csv\")\n",
    "    per_source.to_csv(per_source_out)\n",
    "    print(\"Saved per-source emotion counts to\", per_source_out)\n",
    "else:\n",
    "    print(\"No source column found. You can set a source column to get per-platform stats.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4ef1f-a4d9-4df6-a145-3610da153c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - exports and simple QA\n",
    "# top example posts for each label (for manual inspection)\n",
    "examples = {}\n",
    "for label in EMOTION_LABELS:\n",
    "    subset = final[final[\"pred_label\"] == label]\n",
    "    examples[label] = subset[[\"clean_post_text\", \"raw_reactions\", \"date\"]].head(10)\n",
    "\n",
    "# Save examples in a JSON file for quick review\n",
    "examples_out = os.path.join(OUTPUT_DIR, \"top_examples_per_label.json\")\n",
    "# convert datetimes to string for JSON\n",
    "def to_serializable(df):\n",
    "    d = df.copy()\n",
    "    if \"date\" in d.columns:\n",
    "        d[\"date\"] = d[\"date\"].astype(str)\n",
    "    return d.to_dict(orient=\"records\")\n",
    "\n",
    "json_examples = {k: to_serializable(v) for k,v in examples.items()}\n",
    "with open(examples_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_examples, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved example posts per label to\", examples_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92d9e7-86fb-49df-bfe9-b6e4ba6651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 - short reproducibility notes (printed)\n",
    "print(\"\"\"\n",
    "REPRODUCIBILITY NOTES:\n",
    "- Model dir: {}\n",
    "- Raw merged CSV: {}\n",
    "- Preprocessed CSV: {}\n",
    "- Detailed predictions: {}\n",
    "- Monthly/yearly: {} , {}\n",
    "- Examples per label: {}\n",
    "Make sure model.label order matches EMOTION_LABELS defined in the notebook.\n",
    "\"\"\".format(MODEL_DIR, raw_out, processed_out, final_out, monthly_out if 'monthly_out' in globals() else 'N/A', yearly_out if 'yearly_out' in globals() else 'N/A', examples_out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
